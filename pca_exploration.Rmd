---
title: "Exploratory Data Analysis of Human Activity Recognition Data Set"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Motivation

The human activity data consists of 561 features. Using all of these features in a predictive modeling procedure can be computationally tedious. On the other hand, removing features can result in losing vital information that can describe the process that generated the dependent variables.

Instead of feature removal, trying to derive a new and reduced set of features from the existing set, can infact help us reduce computational efforts while maintaining informational integrity. This process is called feature extraction, with PCA being one of them.



## PCA Algorithm function

PCA is short for Principal Component Analysis. The original set of features are combined and linearly transformed into principal components. 
The set of principal components that explain the data effectively ,is chosen as the new feature set. The PCA process can be explained briefly in the following steps.

Let *X* be a matrix that represents the set of features that we are running PCA on. Each column in this matrix represents a feature.

*  __Step 1__ : Subtract each column by its mean and divide it by the standard deviation.
*  __Step 2__ : Let the new normalized matrix be *Z* . Using this normalized matrix , calculate the covariance matrix ie Z<sup>T</sup>Z









```{r}

pca_function <- function(x,k){
  
  # x is the input data frame
  ## Step 1 Normalize each column
  
  x_scaled <- scale(x)
  ## Convert the data frame to a matrix
  
  x_mat <- as.matrix(x_scaled)
  
  ## Calculate the covariance matrix
  
  cov_x <- t(x_mat) %*% x_mat
  
  
  ## Eigen Value decomposition
  
  decomp_eigen <- eigen(cov_x)
  
  eigen_values <- decomp_eigen$values
  
  eigen_vectors <- decomp_eigen$vectors

  
  # choose top k eigen values and explain the variance
  
  variance_explained <- sum(eigen_values[1:k])/sum(eigen_values)
  
  ## calculate the projected vectors
  
  
  projected_vectors <- x_mat %*% eigen_vectors
  projected_vectors <- projected_vectors[1:nrow(projected_vectors),1:k]
  return(list(projected_vectors=projected_vectors,variance_explained=variance_explained))
}




```



## Choosing the Best K


```{r}

## removing the redundant columns

data_raw_pca <- data_raw %>% select(-rn,-activity)

# possible values of K


variance_data <- list()

for(k in 1:ncol(data_raw_pca)){
  
  variance_temp <- pca_function(data_raw_pca,k)$variance_explained
  dat <- data.frame(k=k,variance=variance_temp)
  
  variance_data[[k]] <- dat
  
  
  
  
}


final_data <- do.call(rbind,variance_data)

```



```{r}


final_data %>%
  ggplot(aes(x=k,y=variance))+geom_line()


final_data %>%
  filter(variance<0.91) %>%
  arrange(desc(variance)) %>%
  head(1)


```




```{r}
pca_final <- pca_function(data_raw_pca,68)


data_pca <- as.data.frame(pca_final$projected_vectors)

data_pca$activity <- data_raw$activity
```




```{r}

library(nnet)
data_pca$activity <- relevel(data_pca$activity,ref='SITTING')

model_logistic <- multinom(activity~.,data=data_pca)
broom::tidy(model_logistic)

test <- predict(model_logistic,data=data_pca %>% select(-activity))


real_pred <- data.frame(real=data_pca$activity,test=as.character(test))
caret::confusionMatrix(real_pred$real,real_pred$test)


```