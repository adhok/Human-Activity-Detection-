---
title: "Exploratory Data Analysis of Human Activity Recognition Data Set"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Motivation

The human activity data consists of 561 features. Using all of these features in a predictive modeling procedure can be computationally tedious. On the other hand, removing features can result in losing vital information that can describe the process that generated the dependent variables.

Instead of feature removal, trying to derive a new and reduced set of features from the existing set, can infact help us reduce computational efforts while maintaining informational integrity. This process is called feature extraction, with PCA being one of them.



## PCA Algorithm function

PCA is short for Principal Component Analysis. The original set of features are combined and linearly transformed into principal components. 
The set of principal components that explain the data effectively ,is chosen as the new feature set. The PCA process can be explained briefly in the following steps.

Let *X* be a matrix that represents the set of features that we are running PCA on. Each column in this matrix represents a feature.

*  __Step 1__ : Subtract each column by its mean and divide it by the standard deviation.
*  __Step 2__ : Let the new normalized matrix be *Z* . Using this normalized matrix , calculate the covariance matrix ie Z<sup>T</sup>Z . Let this covariance matrix be *C*
*  __Step 3__ : Decompose the covariance matrix into eigen values and eigen vectors. Here the `eigen()` function is used. The eigen vector corresponding to the highest eigen value is the direction of maximum variance.

*  __Step 4__ : To acquire the principal components, arrange the eigen values in descending order and reorder the eigen vectors accordingly. This reordering is taken care of by the `eigen()` function.

*  __Step 5__ : Now that we have the directions along which the variance is maximum, we need to project them on the data that we have ,to create the features. This is achieved by multiplying the scaled feature matrix *Z* with the reordered eigen matrix. The explained variance of each component is the eigen value corresponding to that component divided by the sum of eigen values.


These steps are encapsulated via the `pca_function()` that is defined below. This function returns the following

1. The projected vectors

2. The eigen values

3. The directions along which the data is projected.



```{r}

pca_function <- function(x,k){
  
  # x is the input data frame
  ## Step 1 Normalize each column
  
  x_scaled <- scale(x)
  ## Convert the data frame to a matrix
  
  x_mat <- as.matrix(x_scaled)
  
  ## Calculate the covariance matrix
  
  cov_x <- t(x_mat) %*% x_mat
  
  
  ## Eigen Value decomposition
  
  decomp_eigen <- eigen(cov_x)
  
  eigen_values <- decomp_eigen$values
  
  eigen_vectors <- decomp_eigen$vectors

  
  # choose top k eigen values and explain the variance
  
  eigen_values <- eigen_values[1:k]
  
  ## calculate the projected vectors
  
  
  projected_vectors <- x_mat %*% eigen_vectors
  projected_vectors <- projected_vectors[1:nrow(projected_vectors),1:k]
  return(list(projected_vectors=projected_vectors,eigen_values=eigen_values,basis_vectors=eigen_vectors))
}




```



## Choosing the Best K

Its good to visualize the optimal number of principal components by looking how well they explain the data. Consecutive principal components explain lesser and lesser of the variance. 


```{r}

## removing the redundant columns




set.seed(42)

libraries_needed <- c('tidyr','dplyr','ggplot2','caret','purrr','rlang','caret')
lapply(libraries_needed ,require,character.only=TRUE)
data_raw<- read.csv('train.csv',stringsAsFactors = FALSE)


data_raw_pca <- data_raw %>% select(-rn,-activity)




train_index <- sample(1:nrow(data_raw_pca),0.75*nrow(data_raw_pca))

data_raw_pca_train <- data_raw_pca[train_index,]

data_raw_pca_test <- data_raw_pca[-train_index,]
  
pca_decomposed_data <- pca_function(data_raw_pca_train,561)




  

```



```{r}
## Convert to a data frame
options(scipen = 999)

eigen_values_and_vectors <- data.frame(principal_component = 1:ncol(data_raw_pca_train),eigen_values = pca_decomposed_data$eigen_values)


eigen_values_and_vectors %>%
  mutate(var = eigen_values/sum(eigen_values)) %>%
  mutate(cumulative_var = cumsum(var)) %>%
  ggplot(aes(x=principal_component,y=cumulative_var))+
  geom_line()+geom_point()





  




```


The first 100 principal components explain about 95% of the variance in the data. Let's apply this transformation and choose the first 100 principal components. We will then use these principal components in our multinomial logistic regression model. 



```{r}
pca_final_train <- pca_function(data_raw_pca_train,100)


data_pca_train <- as.data.frame(pca_final_train$projected_vectors)

data_pca_train$activity <- as.character(data_raw[train_index,]$activity)
```



Multinomial logistic regression models help in deriving the log odds of an event happening against a base event. This is example, the base event is chosen as `SITTING`.


```{r}

library(nnet)

data_pca_train$activity <- as.factor(data_pca_train$activity)
data_pca_train$activity <- relevel(data_pca_train$activity,ref='SITTING')

model_logistic_pca <- multinom(activity~.,data=data_pca_train)
#broom::tidy(model_logistic_pca)

test <- predict(model_logistic_pca,newdata=data_pca_train %>% select(-activity))


real_pred <- data.frame(real=data_pca_train$activity,test=as.character(test))
caret::confusionMatrix(real_pred$real,real_pred$test)


```

* As per the output, the 98% of the data points were classified properly.
* For the purpose of prediction on the test data, we project the scaled test data onto the principal directions.
* The resulting vectors are then used in the model to generate the predicted values.
* These predicted values are then compared against the test values to evaluate the test accuracy,


## Testing 

```{r}

## testing

## Project scaled test data onto the directons of maximum variance (basis vectors)
data_pca_test <- as.matrix(scale(data_raw_pca_test)) %*% pca_final_train$basis_vectors

data_pca_test <- as.data.frame(data_pca_test)

data_pca_test <- data_pca_test[,1:100]

data_pca_test$activity <- data_raw[-train_index,]$activity
 
test_prediction <- predict(model_logistic_pca,newdata = data_pca_test %>% select(-activity))
real_pred_test <- data.frame(real=data_pca_test$activity,test=as.character(test_prediction))


caret::confusionMatrix(real_pred_test$real,real_pred_test$test)




```

* 94% of test data is correctly predicted.


## 10 folds Cross validation of the approach


Good training and test accuracies are achieved in the previous sections with one set of test and train data. A good way to make sure that the predictive modeling approach is robust is to run it with different combinations of test and train data.

```{r}
random_data <- dplyr::slice(data_raw,sample(1:n()))
test_idx <- round(seq(1,nrow(data_raw),by=nrow(data_raw)/11))
accuracy_df <- data.frame(iteration= as.numeric(),training_score= as.numeric(),testing_score= as.numeric())
for(i in 1:10){
  
  
  ## Training
  
  test_data <- slice(random_data,test_idx[i]:test_idx[i+1])
  train_data <- slice(random_data,-test_idx[i]:-test_idx[i+1])
  pca_final_train <- pca_function(train_data %>% select(-rn,-activity),100)
  
  data_pca_train <- as.data.frame(pca_final_train$projected_vectors)
  data_pca_train$activity <- as.character(train_data$activity)
  #data_pca_train$activity <- relevel(data_pca_train$activity,ref='SITTING')

  model_cv <- multinom(activity~.,data=data_pca_train)
  
  train_predict <- as.character(predict(model_cv,newdata=data_pca_train %>% select(-activity)))
  
  training_score <- sum(as.character(train_predict)==as.character(train_data$activity))/nrow(train_data)
  
  
  
  
  
  ## Testing 
  data_pca_test <- as.matrix(scale(test_data %>% select(-rn,-activity))) %*% pca_final_train$basis_vectors

  data_pca_test <- as.data.frame(data_pca_test)

  data_pca_test <- data_pca_test[,1:100]

  data_pca_test$activity <- test_data$activity
  
  
  
  predict_cv <- as.character(predict(model_cv,newdata=data_pca_test %>% select(-activity)))
  testing_score <- sum(as.character(predict_cv)==as.character(test_data$activity))/nrow(test_data)
  accuracy_df <- rbind(accuracy_df,data.frame(iteration=i,testing_score=testing_score,training_score =training_score ))
  
  

  
  
  
  
  
}







```


```{r}


scaleFUN <- function(x) sprintf("%.f", x)

accuracy_df %>%
  tidyr::gather(type_of_score,value,2:3) %>%
  mutate(type_of_score = ifelse(grepl('training',type_of_score),'Training Score','Testing Score')) %>%
  ggplot(aes(x=iteration,y=value,color=type_of_score))+geom_line()+
  scale_x_continuous(labels = scaleFUN)+
  labs(x='Iteration Number',y='Score')



```

* Training scores are stable and testing scores range from 0.94 to 0.97 which is acceptable for a good multi class classifier.



## Closing remarks

* PCA allows for modeling in a smaller dimension without the loss of vital information. 
* Running a multi class logistic model with PCA yields better testing and training results.








